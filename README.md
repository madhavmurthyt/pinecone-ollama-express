# ğŸ§  Ollama RAG Kit

A lightweight Retrieval-Augmented Generation (RAG) service using **Ollama** (local LLM) and **Pinecone** for vector search. Built for developers and teams who want to integrate AI-powered contextual responses with speed, control, and local inference.

---

## ğŸ”§ Features

- `/query`: Query endpoint that retrieves relevant chunks from Pinecone and generates response using Ollama
- `/ingest`: Utility to parse and embed documents into Pinecone
- Token-level logging and latency tracking (WIP)
- Pluggable design for local vs cloud LLMs (Ollama / OpenAI)
- Ready-to-use REST API endpoints with JSON I/O

---

## ğŸ“ Project Structure
/pinecone-ollama-express
â”œâ”€â”€ routes/
â”‚ â”œâ”€â”€ query.js # Main query handler
â”‚ â””â”€â”€ ingest.js # Vector DB ingestion utility
â”œâ”€â”€ services/
â”‚ â”œâ”€â”€ ollama.js # TBD
â”‚ â””â”€â”€ pinecone.js # Common pinecone index
â”œâ”€â”€ utils/
â”‚ â””â”€â”€ logger.js # TBD
â”œâ”€â”€ .env
â””â”€â”€ server.js


---

## ğŸš€ Quick Start

### 1. Clone + Install

```bash
git clone https://github.com/yourname/pinecone-ollama-express.git
cd pincone-ollama-express
npm install
